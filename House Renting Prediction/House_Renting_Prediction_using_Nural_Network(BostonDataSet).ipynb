{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "House_Renting_Prediction_using_Nural_NetWork.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPUNZLsKJdW1oGMGoP2DIQv"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xF_aBei1_-6"
      },
      "source": [
        "#import libraries\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from sklearn import linear_model\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSwqY1IA4rti"
      },
      "source": [
        "# load boston house rent data set from sklearen dataset\r\n",
        "from sklearn.datasets import load_boston\r\n",
        "boston = load_boston()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnQI4z3P43eG",
        "outputId": "9efd1099-a5d6-4ab8-b5fb-556e403c6615"
      },
      "source": [
        "#convert the data set using data fream\r\n",
        "dataFrame_X = pd.DataFrame(boston.data, columns= boston.feature_names)\r\n",
        "print(dataFrame_X.head(5))\r\n",
        "dataFrame_Y = pd.DataFrame(boston.target)\r\n",
        "print(dataFrame_Y.head(5))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      CRIM    ZN  INDUS  CHAS    NOX  ...  RAD    TAX  PTRATIO       B  LSTAT\n",
            "0  0.00632  18.0   2.31   0.0  0.538  ...  1.0  296.0     15.3  396.90   4.98\n",
            "1  0.02731   0.0   7.07   0.0  0.469  ...  2.0  242.0     17.8  396.90   9.14\n",
            "2  0.02729   0.0   7.07   0.0  0.469  ...  2.0  242.0     17.8  392.83   4.03\n",
            "3  0.03237   0.0   2.18   0.0  0.458  ...  3.0  222.0     18.7  394.63   2.94\n",
            "4  0.06905   0.0   2.18   0.0  0.458  ...  3.0  222.0     18.7  396.90   5.33\n",
            "\n",
            "[5 rows x 13 columns]\n",
            "      0\n",
            "0  24.0\n",
            "1  21.6\n",
            "2  34.7\n",
            "3  33.4\n",
            "4  36.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "0nObgTNdMl6S",
        "outputId": "dabb0182-2aaf-433f-c318-dfd1d7b351ad"
      },
      "source": [
        "dataFrame_X.corr()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>CRIM</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.200469</td>\n",
              "      <td>0.406583</td>\n",
              "      <td>-0.055892</td>\n",
              "      <td>0.420972</td>\n",
              "      <td>-0.219247</td>\n",
              "      <td>0.352734</td>\n",
              "      <td>-0.379670</td>\n",
              "      <td>0.625505</td>\n",
              "      <td>0.582764</td>\n",
              "      <td>0.289946</td>\n",
              "      <td>-0.385064</td>\n",
              "      <td>0.455621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ZN</th>\n",
              "      <td>-0.200469</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.533828</td>\n",
              "      <td>-0.042697</td>\n",
              "      <td>-0.516604</td>\n",
              "      <td>0.311991</td>\n",
              "      <td>-0.569537</td>\n",
              "      <td>0.664408</td>\n",
              "      <td>-0.311948</td>\n",
              "      <td>-0.314563</td>\n",
              "      <td>-0.391679</td>\n",
              "      <td>0.175520</td>\n",
              "      <td>-0.412995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>INDUS</th>\n",
              "      <td>0.406583</td>\n",
              "      <td>-0.533828</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.062938</td>\n",
              "      <td>0.763651</td>\n",
              "      <td>-0.391676</td>\n",
              "      <td>0.644779</td>\n",
              "      <td>-0.708027</td>\n",
              "      <td>0.595129</td>\n",
              "      <td>0.720760</td>\n",
              "      <td>0.383248</td>\n",
              "      <td>-0.356977</td>\n",
              "      <td>0.603800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CHAS</th>\n",
              "      <td>-0.055892</td>\n",
              "      <td>-0.042697</td>\n",
              "      <td>0.062938</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.091203</td>\n",
              "      <td>0.091251</td>\n",
              "      <td>0.086518</td>\n",
              "      <td>-0.099176</td>\n",
              "      <td>-0.007368</td>\n",
              "      <td>-0.035587</td>\n",
              "      <td>-0.121515</td>\n",
              "      <td>0.048788</td>\n",
              "      <td>-0.053929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NOX</th>\n",
              "      <td>0.420972</td>\n",
              "      <td>-0.516604</td>\n",
              "      <td>0.763651</td>\n",
              "      <td>0.091203</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.302188</td>\n",
              "      <td>0.731470</td>\n",
              "      <td>-0.769230</td>\n",
              "      <td>0.611441</td>\n",
              "      <td>0.668023</td>\n",
              "      <td>0.188933</td>\n",
              "      <td>-0.380051</td>\n",
              "      <td>0.590879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RM</th>\n",
              "      <td>-0.219247</td>\n",
              "      <td>0.311991</td>\n",
              "      <td>-0.391676</td>\n",
              "      <td>0.091251</td>\n",
              "      <td>-0.302188</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.240265</td>\n",
              "      <td>0.205246</td>\n",
              "      <td>-0.209847</td>\n",
              "      <td>-0.292048</td>\n",
              "      <td>-0.355501</td>\n",
              "      <td>0.128069</td>\n",
              "      <td>-0.613808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AGE</th>\n",
              "      <td>0.352734</td>\n",
              "      <td>-0.569537</td>\n",
              "      <td>0.644779</td>\n",
              "      <td>0.086518</td>\n",
              "      <td>0.731470</td>\n",
              "      <td>-0.240265</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.747881</td>\n",
              "      <td>0.456022</td>\n",
              "      <td>0.506456</td>\n",
              "      <td>0.261515</td>\n",
              "      <td>-0.273534</td>\n",
              "      <td>0.602339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DIS</th>\n",
              "      <td>-0.379670</td>\n",
              "      <td>0.664408</td>\n",
              "      <td>-0.708027</td>\n",
              "      <td>-0.099176</td>\n",
              "      <td>-0.769230</td>\n",
              "      <td>0.205246</td>\n",
              "      <td>-0.747881</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.494588</td>\n",
              "      <td>-0.534432</td>\n",
              "      <td>-0.232471</td>\n",
              "      <td>0.291512</td>\n",
              "      <td>-0.496996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RAD</th>\n",
              "      <td>0.625505</td>\n",
              "      <td>-0.311948</td>\n",
              "      <td>0.595129</td>\n",
              "      <td>-0.007368</td>\n",
              "      <td>0.611441</td>\n",
              "      <td>-0.209847</td>\n",
              "      <td>0.456022</td>\n",
              "      <td>-0.494588</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.910228</td>\n",
              "      <td>0.464741</td>\n",
              "      <td>-0.444413</td>\n",
              "      <td>0.488676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TAX</th>\n",
              "      <td>0.582764</td>\n",
              "      <td>-0.314563</td>\n",
              "      <td>0.720760</td>\n",
              "      <td>-0.035587</td>\n",
              "      <td>0.668023</td>\n",
              "      <td>-0.292048</td>\n",
              "      <td>0.506456</td>\n",
              "      <td>-0.534432</td>\n",
              "      <td>0.910228</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.460853</td>\n",
              "      <td>-0.441808</td>\n",
              "      <td>0.543993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PTRATIO</th>\n",
              "      <td>0.289946</td>\n",
              "      <td>-0.391679</td>\n",
              "      <td>0.383248</td>\n",
              "      <td>-0.121515</td>\n",
              "      <td>0.188933</td>\n",
              "      <td>-0.355501</td>\n",
              "      <td>0.261515</td>\n",
              "      <td>-0.232471</td>\n",
              "      <td>0.464741</td>\n",
              "      <td>0.460853</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.177383</td>\n",
              "      <td>0.374044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B</th>\n",
              "      <td>-0.385064</td>\n",
              "      <td>0.175520</td>\n",
              "      <td>-0.356977</td>\n",
              "      <td>0.048788</td>\n",
              "      <td>-0.380051</td>\n",
              "      <td>0.128069</td>\n",
              "      <td>-0.273534</td>\n",
              "      <td>0.291512</td>\n",
              "      <td>-0.444413</td>\n",
              "      <td>-0.441808</td>\n",
              "      <td>-0.177383</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.366087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LSTAT</th>\n",
              "      <td>0.455621</td>\n",
              "      <td>-0.412995</td>\n",
              "      <td>0.603800</td>\n",
              "      <td>-0.053929</td>\n",
              "      <td>0.590879</td>\n",
              "      <td>-0.613808</td>\n",
              "      <td>0.602339</td>\n",
              "      <td>-0.496996</td>\n",
              "      <td>0.488676</td>\n",
              "      <td>0.543993</td>\n",
              "      <td>0.374044</td>\n",
              "      <td>-0.366087</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             CRIM        ZN     INDUS  ...   PTRATIO         B     LSTAT\n",
              "CRIM     1.000000 -0.200469  0.406583  ...  0.289946 -0.385064  0.455621\n",
              "ZN      -0.200469  1.000000 -0.533828  ... -0.391679  0.175520 -0.412995\n",
              "INDUS    0.406583 -0.533828  1.000000  ...  0.383248 -0.356977  0.603800\n",
              "CHAS    -0.055892 -0.042697  0.062938  ... -0.121515  0.048788 -0.053929\n",
              "NOX      0.420972 -0.516604  0.763651  ...  0.188933 -0.380051  0.590879\n",
              "RM      -0.219247  0.311991 -0.391676  ... -0.355501  0.128069 -0.613808\n",
              "AGE      0.352734 -0.569537  0.644779  ...  0.261515 -0.273534  0.602339\n",
              "DIS     -0.379670  0.664408 -0.708027  ... -0.232471  0.291512 -0.496996\n",
              "RAD      0.625505 -0.311948  0.595129  ...  0.464741 -0.444413  0.488676\n",
              "TAX      0.582764 -0.314563  0.720760  ...  0.460853 -0.441808  0.543993\n",
              "PTRATIO  0.289946 -0.391679  0.383248  ...  1.000000 -0.177383  0.374044\n",
              "B       -0.385064  0.175520 -0.356977  ... -0.177383  1.000000 -0.366087\n",
              "LSTAT    0.455621 -0.412995  0.603800  ...  0.374044 -0.366087  1.000000\n",
              "\n",
              "[13 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urDRvXxc7ADw"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(dataFrame_X,dataFrame_Y,test_size = 0.33,random_state=42)\r\n"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJaMLW6eAvno"
      },
      "source": [
        "#import the libraries\r\n",
        "import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import LeakyReLU,PReLU,ELU\r\n",
        "from keras.layers import Dropout"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqUhwb85BK2V",
        "outputId": "df57d3c6-87fa-40f2-afe5-030678abb654"
      },
      "source": [
        "#make the model\r\n",
        "nuralNetworkModel = Sequential()\r\n",
        "\r\n",
        "# The Input Layer :\r\n",
        "nuralNetworkModel.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\r\n",
        "\r\n",
        "# The Hidden Layers :\r\n",
        "nuralNetworkModel.add(Dense(256, kernel_initializer='normal',activation='relu'))\r\n",
        "nuralNetworkModel.add(Dense(256, kernel_initializer='normal',activation='relu'))\r\n",
        "nuralNetworkModel.add(Dense(256, kernel_initializer='normal',activation='relu'))\r\n",
        "nuralNetworkModel.add(Dense(256, kernel_initializer='normal',activation='relu'))\r\n",
        "nuralNetworkModel.add(Dense(256, kernel_initializer='normal',activation='relu'))\r\n",
        "\r\n",
        "# The Output Layer :\r\n",
        "nuralNetworkModel.add(Dense(1, kernel_initializer='normal',activation='linear'))\r\n",
        "\r\n",
        "# Compile the network :\r\n",
        "nuralNetworkModel.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\r\n",
        "nuralNetworkModel.summary()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_8 (Dense)              (None, 128)               1792      \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 298,241\n",
            "Trainable params: 298,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pg6_upgyCyX7",
        "outputId": "7bba8220-e633-40af-a7b4-c9fe3c55bacc"
      },
      "source": [
        "nuralNetworkModel.fit(X_train, Y_train,validation_split=0.33, batch_size = 10, epochs = 100)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "23/23 [==============================] - 1s 13ms/step - loss: 13.3077 - mean_absolute_error: 13.3077 - val_loss: 6.0198 - val_mean_absolute_error: 6.0198\n",
            "Epoch 2/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 6.1436 - mean_absolute_error: 6.1436 - val_loss: 5.2552 - val_mean_absolute_error: 5.2552\n",
            "Epoch 3/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 5.5376 - mean_absolute_error: 5.5376 - val_loss: 6.8828 - val_mean_absolute_error: 6.8828\n",
            "Epoch 4/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 6.0067 - mean_absolute_error: 6.0067 - val_loss: 5.4407 - val_mean_absolute_error: 5.4407\n",
            "Epoch 5/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 5.8488 - mean_absolute_error: 5.8488 - val_loss: 5.4841 - val_mean_absolute_error: 5.4841\n",
            "Epoch 6/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.5630 - mean_absolute_error: 5.5630 - val_loss: 4.9436 - val_mean_absolute_error: 4.9436\n",
            "Epoch 7/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.0739 - mean_absolute_error: 6.0739 - val_loss: 5.6866 - val_mean_absolute_error: 5.6866\n",
            "Epoch 8/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.9818 - mean_absolute_error: 5.9818 - val_loss: 5.1044 - val_mean_absolute_error: 5.1044\n",
            "Epoch 9/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.1702 - mean_absolute_error: 5.1702 - val_loss: 6.4095 - val_mean_absolute_error: 6.4095\n",
            "Epoch 10/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.6762 - mean_absolute_error: 6.6762 - val_loss: 5.1713 - val_mean_absolute_error: 5.1713\n",
            "Epoch 11/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 4.8707 - mean_absolute_error: 4.8707 - val_loss: 4.5807 - val_mean_absolute_error: 4.5807\n",
            "Epoch 12/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.0941 - mean_absolute_error: 5.0941 - val_loss: 4.7803 - val_mean_absolute_error: 4.7803\n",
            "Epoch 13/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 5.6196 - mean_absolute_error: 5.6196 - val_loss: 5.2655 - val_mean_absolute_error: 5.2655\n",
            "Epoch 14/100\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 5.3636 - mean_absolute_error: 5.3636 - val_loss: 4.4258 - val_mean_absolute_error: 4.4258\n",
            "Epoch 15/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 5.4502 - mean_absolute_error: 5.4502 - val_loss: 4.7908 - val_mean_absolute_error: 4.7908\n",
            "Epoch 16/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 4.5958 - mean_absolute_error: 4.5958 - val_loss: 4.6312 - val_mean_absolute_error: 4.6312\n",
            "Epoch 17/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.3904 - mean_absolute_error: 4.3904 - val_loss: 6.3630 - val_mean_absolute_error: 6.3630\n",
            "Epoch 18/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 5.9466 - mean_absolute_error: 5.9466 - val_loss: 4.6045 - val_mean_absolute_error: 4.6045\n",
            "Epoch 19/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.9107 - mean_absolute_error: 4.9107 - val_loss: 4.7592 - val_mean_absolute_error: 4.7592\n",
            "Epoch 20/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.4961 - mean_absolute_error: 4.4961 - val_loss: 4.2739 - val_mean_absolute_error: 4.2739\n",
            "Epoch 21/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 5.0398 - mean_absolute_error: 5.0398 - val_loss: 5.6070 - val_mean_absolute_error: 5.6070\n",
            "Epoch 22/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 4.6604 - mean_absolute_error: 4.6604 - val_loss: 4.0836 - val_mean_absolute_error: 4.0836\n",
            "Epoch 23/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 4.1471 - mean_absolute_error: 4.1471 - val_loss: 3.9948 - val_mean_absolute_error: 3.9948\n",
            "Epoch 24/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.0993 - mean_absolute_error: 4.0993 - val_loss: 5.6532 - val_mean_absolute_error: 5.6532\n",
            "Epoch 25/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.6555 - mean_absolute_error: 4.6555 - val_loss: 4.1066 - val_mean_absolute_error: 4.1066\n",
            "Epoch 26/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.1292 - mean_absolute_error: 4.1292 - val_loss: 4.0027 - val_mean_absolute_error: 4.0027\n",
            "Epoch 27/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 4.8215 - mean_absolute_error: 4.8215 - val_loss: 4.3517 - val_mean_absolute_error: 4.3517\n",
            "Epoch 28/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.8669 - mean_absolute_error: 4.8669 - val_loss: 5.7293 - val_mean_absolute_error: 5.7293\n",
            "Epoch 29/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 4.5189 - mean_absolute_error: 4.5189 - val_loss: 4.1073 - val_mean_absolute_error: 4.1073\n",
            "Epoch 30/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.0031 - mean_absolute_error: 4.0031 - val_loss: 4.2625 - val_mean_absolute_error: 4.2625\n",
            "Epoch 31/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.3128 - mean_absolute_error: 4.3128 - val_loss: 3.8434 - val_mean_absolute_error: 3.8434\n",
            "Epoch 32/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.5425 - mean_absolute_error: 4.5425 - val_loss: 3.9740 - val_mean_absolute_error: 3.9740\n",
            "Epoch 33/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 4.2279 - mean_absolute_error: 4.2279 - val_loss: 4.5397 - val_mean_absolute_error: 4.5397\n",
            "Epoch 34/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.0701 - mean_absolute_error: 4.0701 - val_loss: 3.9332 - val_mean_absolute_error: 3.9332\n",
            "Epoch 35/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 3.9926 - mean_absolute_error: 3.9926 - val_loss: 3.9586 - val_mean_absolute_error: 3.9586\n",
            "Epoch 36/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 3.8411 - mean_absolute_error: 3.8411 - val_loss: 3.6486 - val_mean_absolute_error: 3.6486\n",
            "Epoch 37/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.2702 - mean_absolute_error: 4.2702 - val_loss: 5.7262 - val_mean_absolute_error: 5.7262\n",
            "Epoch 38/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.2629 - mean_absolute_error: 5.2629 - val_loss: 5.7574 - val_mean_absolute_error: 5.7574\n",
            "Epoch 39/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.9244 - mean_absolute_error: 4.9244 - val_loss: 4.5012 - val_mean_absolute_error: 4.5012\n",
            "Epoch 40/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 4.0193 - mean_absolute_error: 4.0193 - val_loss: 3.8571 - val_mean_absolute_error: 3.8571\n",
            "Epoch 41/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 4.5400 - mean_absolute_error: 4.5400 - val_loss: 3.6485 - val_mean_absolute_error: 3.6485\n",
            "Epoch 42/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.7248 - mean_absolute_error: 4.7248 - val_loss: 4.1223 - val_mean_absolute_error: 4.1223\n",
            "Epoch 43/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 3.4778 - mean_absolute_error: 3.4778 - val_loss: 3.3753 - val_mean_absolute_error: 3.3753\n",
            "Epoch 44/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 3.3482 - mean_absolute_error: 3.3482 - val_loss: 3.7848 - val_mean_absolute_error: 3.7848\n",
            "Epoch 45/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.2729 - mean_absolute_error: 4.2729 - val_loss: 3.6600 - val_mean_absolute_error: 3.6600\n",
            "Epoch 46/100\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 3.7304 - mean_absolute_error: 3.7304 - val_loss: 3.4510 - val_mean_absolute_error: 3.4510\n",
            "Epoch 47/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 3.4570 - mean_absolute_error: 3.4570 - val_loss: 4.6263 - val_mean_absolute_error: 4.6263\n",
            "Epoch 48/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.8164 - mean_absolute_error: 4.8164 - val_loss: 5.2766 - val_mean_absolute_error: 5.2766\n",
            "Epoch 49/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 4.4841 - mean_absolute_error: 4.4841 - val_loss: 5.6507 - val_mean_absolute_error: 5.6507\n",
            "Epoch 50/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.0697 - mean_absolute_error: 4.0697 - val_loss: 3.5090 - val_mean_absolute_error: 3.5090\n",
            "Epoch 51/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 3.2799 - mean_absolute_error: 3.2799 - val_loss: 4.3344 - val_mean_absolute_error: 4.3344\n",
            "Epoch 52/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 3.7800 - mean_absolute_error: 3.7800 - val_loss: 3.3524 - val_mean_absolute_error: 3.3524\n",
            "Epoch 53/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 3.1654 - mean_absolute_error: 3.1654 - val_loss: 4.1390 - val_mean_absolute_error: 4.1390\n",
            "Epoch 54/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 3.5353 - mean_absolute_error: 3.5353 - val_loss: 3.2079 - val_mean_absolute_error: 3.2079\n",
            "Epoch 55/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 3.1805 - mean_absolute_error: 3.1805 - val_loss: 3.3942 - val_mean_absolute_error: 3.3942\n",
            "Epoch 56/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 3.4590 - mean_absolute_error: 3.4590 - val_loss: 3.1238 - val_mean_absolute_error: 3.1238\n",
            "Epoch 57/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 3.1282 - mean_absolute_error: 3.1282 - val_loss: 3.5289 - val_mean_absolute_error: 3.5289\n",
            "Epoch 58/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 3.1306 - mean_absolute_error: 3.1306 - val_loss: 3.2818 - val_mean_absolute_error: 3.2818\n",
            "Epoch 59/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 3.6320 - mean_absolute_error: 3.6320 - val_loss: 5.0098 - val_mean_absolute_error: 5.0098\n",
            "Epoch 60/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 3.3456 - mean_absolute_error: 3.3456 - val_loss: 3.2524 - val_mean_absolute_error: 3.2524\n",
            "Epoch 61/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 3.3946 - mean_absolute_error: 3.3946 - val_loss: 4.4987 - val_mean_absolute_error: 4.4987\n",
            "Epoch 62/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 4.6141 - mean_absolute_error: 4.6141 - val_loss: 4.7292 - val_mean_absolute_error: 4.7292\n",
            "Epoch 63/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.7989 - mean_absolute_error: 4.7989 - val_loss: 3.3922 - val_mean_absolute_error: 3.3922\n",
            "Epoch 64/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 3.4081 - mean_absolute_error: 3.4081 - val_loss: 3.0099 - val_mean_absolute_error: 3.0099\n",
            "Epoch 65/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 3.1704 - mean_absolute_error: 3.1704 - val_loss: 3.4855 - val_mean_absolute_error: 3.4855\n",
            "Epoch 66/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 3.4224 - mean_absolute_error: 3.4224 - val_loss: 3.3764 - val_mean_absolute_error: 3.3764\n",
            "Epoch 67/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 3.0078 - mean_absolute_error: 3.0078 - val_loss: 3.2119 - val_mean_absolute_error: 3.2119\n",
            "Epoch 68/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 3.0248 - mean_absolute_error: 3.0248 - val_loss: 2.9826 - val_mean_absolute_error: 2.9826\n",
            "Epoch 69/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 3.6716 - mean_absolute_error: 3.6716 - val_loss: 4.0889 - val_mean_absolute_error: 4.0889\n",
            "Epoch 70/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 3.1995 - mean_absolute_error: 3.1995 - val_loss: 3.0695 - val_mean_absolute_error: 3.0695\n",
            "Epoch 71/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 3.0969 - mean_absolute_error: 3.0969 - val_loss: 4.4318 - val_mean_absolute_error: 4.4318\n",
            "Epoch 72/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 4.1580 - mean_absolute_error: 4.1580 - val_loss: 3.2255 - val_mean_absolute_error: 3.2255\n",
            "Epoch 73/100\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 2.9901 - mean_absolute_error: 2.9901 - val_loss: 3.3669 - val_mean_absolute_error: 3.3669\n",
            "Epoch 74/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 2.9558 - mean_absolute_error: 2.9558 - val_loss: 3.0038 - val_mean_absolute_error: 3.0038\n",
            "Epoch 75/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.0415 - mean_absolute_error: 4.0415 - val_loss: 3.3006 - val_mean_absolute_error: 3.3006\n",
            "Epoch 76/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 2.9737 - mean_absolute_error: 2.9737 - val_loss: 3.5925 - val_mean_absolute_error: 3.5925\n",
            "Epoch 77/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 3.5795 - mean_absolute_error: 3.5795 - val_loss: 3.8648 - val_mean_absolute_error: 3.8648\n",
            "Epoch 78/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 3.1870 - mean_absolute_error: 3.1870 - val_loss: 3.7864 - val_mean_absolute_error: 3.7864\n",
            "Epoch 79/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 3.3319 - mean_absolute_error: 3.3319 - val_loss: 3.2248 - val_mean_absolute_error: 3.2248\n",
            "Epoch 80/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 3.1800 - mean_absolute_error: 3.1800 - val_loss: 2.8760 - val_mean_absolute_error: 2.8760\n",
            "Epoch 81/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 3.0470 - mean_absolute_error: 3.0470 - val_loss: 3.5866 - val_mean_absolute_error: 3.5866\n",
            "Epoch 82/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 3.1428 - mean_absolute_error: 3.1428 - val_loss: 3.0356 - val_mean_absolute_error: 3.0356\n",
            "Epoch 83/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 3.4446 - mean_absolute_error: 3.4446 - val_loss: 3.1265 - val_mean_absolute_error: 3.1265\n",
            "Epoch 84/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 2.9707 - mean_absolute_error: 2.9707 - val_loss: 3.4094 - val_mean_absolute_error: 3.4094\n",
            "Epoch 85/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 3.1639 - mean_absolute_error: 3.1639 - val_loss: 3.0797 - val_mean_absolute_error: 3.0797\n",
            "Epoch 86/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 3.1555 - mean_absolute_error: 3.1555 - val_loss: 4.4851 - val_mean_absolute_error: 4.4851\n",
            "Epoch 87/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 3.5458 - mean_absolute_error: 3.5458 - val_loss: 3.4940 - val_mean_absolute_error: 3.4940\n",
            "Epoch 88/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 3.7507 - mean_absolute_error: 3.7507 - val_loss: 3.5965 - val_mean_absolute_error: 3.5965\n",
            "Epoch 89/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 3.9677 - mean_absolute_error: 3.9677 - val_loss: 3.1448 - val_mean_absolute_error: 3.1448\n",
            "Epoch 90/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 3.3815 - mean_absolute_error: 3.3815 - val_loss: 3.0541 - val_mean_absolute_error: 3.0541\n",
            "Epoch 91/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 3.0184 - mean_absolute_error: 3.0184 - val_loss: 2.8812 - val_mean_absolute_error: 2.8812\n",
            "Epoch 92/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 2.8358 - mean_absolute_error: 2.8358 - val_loss: 3.4655 - val_mean_absolute_error: 3.4655\n",
            "Epoch 93/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 3.0966 - mean_absolute_error: 3.0966 - val_loss: 2.8812 - val_mean_absolute_error: 2.8812\n",
            "Epoch 94/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 3.0421 - mean_absolute_error: 3.0421 - val_loss: 3.8113 - val_mean_absolute_error: 3.8113\n",
            "Epoch 95/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 3.1955 - mean_absolute_error: 3.1955 - val_loss: 3.2409 - val_mean_absolute_error: 3.2409\n",
            "Epoch 96/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 2.5088 - mean_absolute_error: 2.5088 - val_loss: 3.4058 - val_mean_absolute_error: 3.4058\n",
            "Epoch 97/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 3.2081 - mean_absolute_error: 3.2081 - val_loss: 3.5397 - val_mean_absolute_error: 3.5397\n",
            "Epoch 98/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 3.4389 - mean_absolute_error: 3.4389 - val_loss: 3.6462 - val_mean_absolute_error: 3.6462\n",
            "Epoch 99/100\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 3.2014 - mean_absolute_error: 3.2014 - val_loss: 2.9558 - val_mean_absolute_error: 2.9558\n",
            "Epoch 100/100\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 2.8764 - mean_absolute_error: 2.8764 - val_loss: 3.3652 - val_mean_absolute_error: 3.3652\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f962e924a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlspUZHcC5Lr"
      },
      "source": [
        "predictValues = nuralNetworkModel.predict(X_test)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "DYTuX1R0DO8a",
        "outputId": "580976d2-8e00-44a7-e15b-e9002b610adc"
      },
      "source": [
        "plt.scatter(Y_test,predictValues)\r\n",
        "from sklearn import metrics\r\n",
        "print('MAE:', metrics.mean_absolute_error(Y_test, predictValues))\r\n",
        "print('MSE:', metrics.mean_squared_error(Y_test, predictValues))\r\n",
        "print('RMSE:', np.sqrt(metrics.mean_squared_error(Y_test, predictValues)))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAE: 2.935168636344864\n",
            "MSE: 19.527232915303273\n",
            "RMSE: 4.418962877791945\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}